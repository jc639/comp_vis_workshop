{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af2e2c-a69a-4207-a5b6-cd42cb1bcd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP CELL make sure to run this first.\n",
    "!pip install -qq numpy==1.25.2\n",
    "!pip install -qq matplotlib==3.7.3\n",
    "!pip install torch==2.0.1\n",
    "!pip install torchvision==0.15.2\n",
    "\n",
    "!wget https://raw.githubusercontent.com/jc639/comp_vis_workshop/main/dataloading/__init__.py?_sm_au_=iVV23Qr5WPjQ728QpGsWvKttvN1NG -O dataloading.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f451e-83bc-4317-8944-b3d65869832e",
   "metadata": {},
   "source": [
    "## Add the dataset first before starting this notebook!\n",
    "\n",
    "Upload the dataset zip folder we have given you and unzip with the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9a3f9-488e-430a-accf-c41ac15db976",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ./data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745a707-5855-4329-8081-58c1e1346715",
   "metadata": {},
   "source": [
    "# Running and Evaluating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0a8b7-6884-4f1e-96b0-e1add970fba4",
   "metadata": {},
   "source": [
    "For this workshop we will be training a model on the some of the labelled data, and looking at the results of training a model. Firstly, we will also go through some other ways we could use computer vision methods with the data and what that would require.\n",
    "\n",
    "## What type of model to use?\n",
    "\n",
    "Depending on what you want to achieve there are different families of computer vision models that are designed for different tasks. The most common supervised machine learning tasks in computer vision are classification, object detection and segmentation. We want to look in a bit more detail about what those tasks involve and when you would use them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42ab3a-2995-482e-96b6-1e06f19f608d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "***\n",
    "\n",
    "### Classification\n",
    "\n",
    "Classification is the type of task we will complete with the data we have labelled. Typically for a given image you want to produce a single or multi labels from a set of predetermined labels that fit your task. You are not concerned with where your classes are located in the image, you just want to categorise the whole image. \n",
    "\n",
    "![classification.png](./images/classification.png)\n",
    "\n",
    "Some use cases where you might want classification could include:\n",
    "- Enhancing metadata for unsorted and unlabelled images by allowing you to determine what is in the images from a predetermined set of classes.\n",
    "- Deciding whether images of objects from a manufacturing line contain defects or not.\n",
    "\n",
    "\n",
    "Some models you might use for classification are:\n",
    "- Resnets/ ResNexts\n",
    "- Efficient Nets\n",
    "- ConvNext\n",
    "- Vision Transformer\n",
    "- Swin Transformers\n",
    "\n",
    "To see the available pretrained models from torchvision see here:\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html#classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d28cc-f681-46cc-964d-cb1c2a16dc20",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Object Detection\n",
    "\n",
    "For object detection we have a list of classes but we would like to be able to determine where they are located in images. Most commonly object detection is done by producing bounding boxes - the boxes have a classification assigned to them, and describe where that object is in the image. Given our dataset of animals with some relabelling we could frame the problem as an object detection task by drawing bounding boxes around the cats/dogs/birds - this may be useful if we need to know exactly where the animals are.\n",
    "\n",
    "![object_detection.png](./images/object_detection.png)\n",
    "\n",
    "Some other use cases of object detection:\n",
    "- Automatic cropping of images to objects of interest, for example in an optical character recognition project we might want to first find the object with the text we want to read before we send it to an OCR model. \n",
    "- Safety or crowdedness detection, we could use a person object detector to determine how crowded a location is for example.\n",
    "- Automatic measurement in images, for example we may want to measure the size of objects in images obtained from microscopes at a given magnification. \n",
    "\n",
    "Some common models you can use are:\n",
    "- YOLO\n",
    "- SSD\n",
    "- Faster R-CNN\n",
    "- Retina Net\n",
    "- DETR\n",
    "\n",
    "To see available pretrained models from pytorch see here:\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html#object-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c0499-ac1b-437b-9d62-2cbf13514a5f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Segmentation\n",
    "\n",
    "Segmentation takes object detection a step further and the goal is to assign individual pixels to a class from a predetermined list of classes. This is useful when we want precise detections and measurements from our images. For example with our pill dataset we saw in LabelStudio imagine if pills were only discarded if more than 5% of a pill was scratched, to determine what percentage of the pill is defective we would need to have a good segmentatation mask of the pill and the defect.\n",
    "\n",
    "![segmentation.png](./images/segmentation.png)\n",
    "\n",
    "Its important to note there are two flavours of segmentation:\n",
    "- Semantic -  cannot distinguish between different instances in the same category, i.e. all chairs are marked blue as in the example below.\n",
    "- Instance - can distinguish between different instances of the same categories, i.e. different chairs are distinguished by different colours.\n",
    "\n",
    "![instance_vs_semantic.png](./images/instance_vs_semantic.png)\n",
    "\n",
    "\n",
    "Some common models for segmentation are:\n",
    "- UNET (semantic)\n",
    "- FCN (semantic)\n",
    "- Mask R-CNN (instance)\n",
    "\n",
    "See here for some pretrained models from pytorch:\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html#semantic-segmentation\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html#instance-segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d841d-4293-4b8f-8abf-17e6f30ca546",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "For this workshop we will be training a classification model using a cat, dog and bird dataset as seen in the workshop on dataloaders. \n",
    "\n",
    "Here we have the classes:\n",
    "- 'cat'\n",
    "- 'dog'\n",
    "- 'bird'\n",
    "\n",
    "The goal of our model is when given a image to be able to return which animal the image contains. First let's get started with the model - here we will be using a resnet18, a type of neural network architecture designed for images. For the moment we don't really need to understand the actual operations inside the model simply that for a single image it takes in a image tensor and transforms it to a vector of numbers that represent the prediction of the output class:\n",
    "\n",
    "![model_throughput.png](./images/model_throughput.png)\n",
    "\n",
    "1. We have a input image which is really just a matrix of numbers that are the pixel values with **shape=(number of channels, height, width)**\n",
    "2. This input goes through the model. We can really just think of the model as a function, but the key to training is that we update the function based on its error compared to the correct labels. \n",
    "3. The model produces an array of unnormalised scores (also known as logits). In this use case we have **three** classes so the unnormalised scores have a **shape=(3, 1)**. The positions in this array correspond to label outputs the first being the Bird node, the seconds the Dog node and the third the Cat node.\n",
    "4. As we want to make a single classification (one label per image) we put the scores through the softmax function which in code is `softmax(x) == np.exp(x)/sum(np.exp(x))` where `x` is the unnormalised scores. This functions bounds the numbers in the array to between 0-1 and the array sums to 1. In this case the Cat node has the highest value and that is the models prediction for this input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9d3ab-1ff5-45db-b451-8948a0b4aabb",
   "metadata": {},
   "source": [
    "#### The model\n",
    "\n",
    "Let's write some code to get us a model to start with. Here we are finetuning a pretrained model which means we are taking a model that has been trained on some other dataset, modifying the number of output nodes and then doing some training on our own dataset. If you can this is often a good way to start as you need less data/training to get some results to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba933d1-6972-44ae-94e3-1ad0c3bb93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet18, resnet\n",
    "\n",
    "class CVModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(*list(resnet18(weights=resnet.ResNet18_Weights.DEFAULT).children())[:-1])\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.classifier = nn.Linear(512, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.classifier(x)\n",
    "        \n",
    "\n",
    "model = CVModel(n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e7a19-5085-4632-b590-a9cceefed8a4",
   "metadata": {},
   "source": [
    "To explain the code a little bit further what we have done here is used a known architecture for the 'backbone' portion of the model, and we have instructed it to load the weights that were obtained by training on ImageNet, a common classification dataset. However we don't want the final output layer from ImageNet as it has 1000 classes, so we have created a new final 'classifier' layer that outputs our desired number of classes. \n",
    "\n",
    "The `__init__` method defines the layers in our model and the `forward` method defines how they are applied to a batch of images, by defining a `forward` method we can then use our class like a function and apply `model(..)` to tensors representing a batch of images. \n",
    "\n",
    "### What is a convolutional neural network?\n",
    "\n",
    "The backbone portion of a our network is interchangeable, it simply has to take a image shaped tensor and transform it through a series of mathematical operations (otherwise known as neural network layers) until we have a vector that can represent our class prediction. In this case we have chosen to use a model that comes from a group of architectures known as convolutional neural networks.\n",
    "\n",
    "Although it is not really necessary to understand the model in detail, only that it processes an image and that is able to be changed by training, we want to just briefly go over convolutions. \n",
    "\n",
    "A convolution is essentially a filter, whereby we have a grid of values (for example 3x3 array) known as a kernel that we slide over pixel array and sum the values. These summed values go into a new array like so. This example shows a grayscale image with shape *64x64x1* as it is bit easier to visualise in a single channel image:\n",
    "\n",
    "![](./images/convlayer_detailedview_demo.gif)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "This looks something like this for the input with a RGB 3 channel image, here we actually apply a 3-dimensional kernel that is *3x3x3*:\n",
    "\n",
    "![](./images/convlayer_overview_demo.gif)\n",
    "\n",
    "The key is that the values of the convolutional kernels are updated in the training process and which allow them to get better at extracting meaningful features for the given task. To be useful we have don't just train a single filter at a given layer, we want to use multiple kernels - the outputs of these kernels are then stacked and are the input for the next layer which can be more convolutions!\n",
    "\n",
    "***\n",
    "\n",
    "Lets run a 'test image' which has the shape the model expects through the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaee2dd-1a07-476b-b709-07ec02063eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why is the shape (1, 3, 128, 128) rather than n_channels (3), height (128), width (128)?\n",
    "# the model expects batchs and this is just a batch size of 1\n",
    "# what happens to what is returned by the following code cell if you change 1 to 5?\n",
    "test_img = torch.rand((1, 3, 128, 128))\n",
    "print(test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c1bed-ccbb-4399-869b-85d275b72cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to evaluation mode - this affects some layers in the model\n",
    "# and we should call this before we use the model outside of training\n",
    "model.eval()\n",
    "\n",
    "# what is this no_grad()?\n",
    "# we are telling pytorch not to track gradients\n",
    "# as it requires less memory and gradients are\n",
    "# only needed during training\n",
    "with torch.no_grad():\n",
    "    logits = model(test_img)\n",
    "\n",
    "print(logits.softmax(axis=1))\n",
    "logits.softmax(axis=1).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431cbad6-9479-4e11-8267-c4e31026e147",
   "metadata": {},
   "source": [
    "## Dataset and Dataloaders\n",
    "\n",
    "As we saw in previous lesson we need to efficiently load and batch the data. We will mostly be copying what we did there but we will just add a few more transforms. For the training dataset we will randomly flip the image on the horizontal access with a 50% probability each time we load it, and then for both training and validation we need convert the images to tensors. Additionally as we are using a pretrained model we need to scale the data according the statistics that the model was originally trained with. \n",
    "\n",
    "First let's implement a function to get a Dataset and Dataloader from a folder with the structure we saw in the previous exercise. Here we are importing the `CustomDataset` class and `SquareImage` transform from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cdb1d1-7480-4e55-b3f1-e7e16d0e5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataloading import CustomDataset, SquareImage\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(root_folder, batchsize=16, class_to_idx=None, transforms=None):\n",
    "    \"\"\"Function to get and return a dataloader for folder\"\"\"\n",
    "    items = []\n",
    "    if class_to_idx is None:\n",
    "        class_to_idx = {}\n",
    "\n",
    "    class_folders = [f for f in os.listdir(root_folder) if not 'DS_Store' in f]\n",
    "    for i, class_ in enumerate(class_folders):\n",
    "        if class_ not in class_to_idx:\n",
    "            class_to_idx[class_] = i\n",
    "        folder_path = os.path.join(root_folder, class_)\n",
    "        imgs = [(os.path.join(folder_path, f), class_) for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "        items.extend(imgs)\n",
    "    \n",
    "    ds = CustomDataset(items, class_to_idx, transforms=transforms)\n",
    "    dl = DataLoader(ds, shuffle=True, batch_size=batchsize)\n",
    "    return ds, dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a3d26-7d55-45fc-a5f1-82989c069995",
   "metadata": {},
   "source": [
    "When we load the images we need apply our transforms. We should have two distinct transform pipelines for our training and validation datasets as our training transform will have the random flipping. \n",
    "\n",
    "For the **training** transforms we have:\n",
    "1. Our custom transform `SquareImage` which pads our image in either the height or width direction to make it square.\n",
    "2. `Resize` which resizes to the desired size. If a single number is given both the height and width are resized to that value.\n",
    "3. `RandomHorizontalFlip` which will flip the image the image on the horizontal axis with a 50% probability by default.\n",
    "4. `ToImageTensor` and `ConvertImageDtype` deal with converting a PIL Image to Pytorch tensor with the correct type, the second step scales the data to between 0 - 1.\n",
    "5. `Normalize` normalises the input data with  `output[channel] = (input[channel] - mean[channel]) / std[channel]` where channel represents the index for R, G or B. The values we have given are there because these are the channel values calculated on ImageNet, and our model has been pretrained on ImageNet **so it is expecting input in this distribution range**.\n",
    "\n",
    "The **validation** pipeline is almost the same but we have removed the `RandomHorizontalFlip`. Why might this be? Well, when we load images for validation we want it to be exactly the same each time so that any time we calculate the validation metrics it is a fair comparison between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e54383-7265-497d-9a7a-405f75552b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "train_transforms = v2.Compose(\n",
    "    [\n",
    "        SquareImage(),\n",
    "        v2.Resize(224),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.ToImageTensor(),\n",
    "        v2.ConvertImageDtype(),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = v2.Compose(\n",
    "    [\n",
    "        SquareImage(),\n",
    "        v2.Resize(224),\n",
    "        v2.ToImageTensor(),\n",
    "        v2.ConvertImageDtype(),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds, train_dl = get_dataloader('data/train', transforms=train_transforms)\n",
    "# we need to pass the train_ds.class_to_idx to the validation dataset to \n",
    "# make sure the classes have the same mapping for train and validation datasets\n",
    "val_ds, val_dl = get_dataloader('data/val/', transforms=val_transforms, class_to_idx=train_ds.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3614af-7e99-405d-a2db-9aafbd0a3b17",
   "metadata": {},
   "source": [
    "The Dataloaders are iterables (we can use `for ... in` to run through each batch), so lets load the first input (`x`)  and the labels (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b577866-a9a0-4d98-af41-71ea439252ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dl:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e6431-b6a6-4534-a4d9-3edd33288c55",
   "metadata": {},
   "source": [
    "We have a batch of 16 images (each image is 3 channels of height=224 _x_ width=224) and an associated 16 class labels which are a mix of the difference classes we have in our dataset.\n",
    "\n",
    "***\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Pytorch doesn't provide any utility functions for training models as it leaves the creation of a training loop up to the user, this makes it highly customisable but you may find you are writing similar code for a variety of tasks.\n",
    "\n",
    "We don't intend to do a full overview of model training as that can be found elsewhere in much more detail (see this [YouTube playlist](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) for a good overview of each step) but we will briefly go over the following code now.\n",
    "\n",
    "The code trains a neural network by showing it a bunch of examples (the training data) and telling it what the correct output is for each example. The neural network uses this information to learn how to predict the correct output for new examples. The code does this by repeatedly showing the neural network batches of training examples and adjusting the weights of the neural network slightly after each batch. This process is called backpropagation. Over time, the neural network learns to predict the correct output for new examples more accurately.\n",
    "\n",
    "#### Code Line by Line\n",
    "\n",
    "The code works by first defining the learning rate, optimizer, and loss function. \n",
    "- The learning rate (`LEARNING_RATE`) controls how much the weights of the neural network are updated each iteration.\n",
    "- The optimizer (`Adam`) is an algorithm that updates the weights in a way that minimizes the loss function.\n",
    "- The loss function (`CrossEntropy`) measures the error and returns how well the neural network is performing on the training data.\n",
    "\n",
    "Next, the code enters a training loop. For each epoch (iteration), the code does the following:\n",
    "\n",
    "1. Sets the model to training mode.\n",
    "2. Iterates over the training data, batch by batch.\n",
    "3. For each batch, the code\n",
    "    - Zeroes out the gradients of the model.\n",
    "    - Forwards the batch of images through the model to get the predictions.\n",
    "    - Calculates the loss between the predictions and the ground truth labels.\n",
    "    - Backpropagates the loss through the model to calculate the gradients of the weights.\n",
    "    - Updates the weights of the model using the optimizer.\n",
    "    - Prints the training loss for the epoch.\n",
    "4. Evaluates the model on the validation data (if specified).\n",
    "\n",
    "#### Backpropagation\n",
    "Backpropagation is the algorithm used to update the weights of the neural network. It works by propagating the error from the output layer of the network back to the input layer. At each layer, the error is used to calculate the gradients of the weights. The gradients tell us how to change the weights to reduce the error/loss of the model and are used to update the weights.\n",
    "\n",
    "The backpropagation algorithm is very efficient and is able to update the weights of even very large neural networks. It is one of the key reasons why neural networks are able to learn complex tasks.\n",
    "\n",
    "#### Example\n",
    "Suppose we are training a neural network to classify images of cats and dogs. The neural network has two outputs, one for cats and one for dogs. We feed the neural network a batch of images and get the predictions. We then calculate the loss between the predictions and the ground truth labels.\n",
    "\n",
    "![model_training.png](./images/model_training.png)\n",
    "\n",
    "The backpropagation algorithm then propagates the error from the output layer of the network back to the input layer. At each layer, the error is used to calculate the gradients of the weights. \n",
    "\n",
    "After a complete backpropagation pass, all the weights of the neural network have been updated slightly. We can then repeat the process with another batch of images. Over time, the neural network will learn to classify images of cats and dogs more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371e21a-cc0a-4947-a055-86b864a53208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "opt = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_dl, val_dl, opt, loss_f, n_epochs, eval=True):\n",
    "    \"\"\"Training model for number of epochs specified\"\"\"\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        print(f'Epoch {epoch + 1}:')\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for imgs, labels in tqdm(train_dl):\n",
    "            opt.zero_grad()\n",
    "            output = model(imgs)\n",
    "            loss = loss_f(output, labels)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            opt.step()\n",
    "        print(f'Loss:\\t {running_loss / len(train_dl):.2f}')\n",
    "            \n",
    "        if eval:\n",
    "            evaluate(model=model, val_dl=val_dl, loss_f=loss_f)\n",
    "\n",
    "def evaluate(model, val_dl, loss_f):\n",
    "    \"\"\"Evaluate the model with a validation dataloader\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        for imgs, labels in tqdm(val_dl):\n",
    "            logits = model(imgs)\n",
    "            proba = logits.softmax(axis=1)\n",
    "            preds = proba.argmax(axis=1)\n",
    "            acc += (preds == labels).sum()\n",
    "            loss += loss_f(logits, labels).item()\n",
    "        print(f'Validation loss:\\t {loss / len(val_dl):.2f}')\n",
    "        print(f'Validation Accuracy:\\t {(acc / len(val_ds))*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550d3d9-9210-414e-8d85-a69e8316348d",
   "metadata": {},
   "source": [
    "Let's train for 5 epochs and see what happens to the loss and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19022af-4137-43bb-a9d9-21c8eba41765",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model, train_dl=train_dl, val_dl=val_dl, \n",
    "      opt=opt, loss_f=loss_func, n_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5180c5-2a7d-4187-9bc2-940aa249e0f4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Let's checkout some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393929e0-3e8d-4fcd-9f1c-2188840a587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def predict(model, x):\n",
    "    \"\"\"Predict given a tensor\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        softmax = logits.softmax(axis=1)\n",
    "        arg_max = softmax.argmax(axis=1)\n",
    "    return logits, softmax, arg_max\n",
    "\n",
    "\n",
    "def predict_filepath(model, img_path, transforms, idx_to_class):\n",
    "    \"\"\"Predict given an image filepath\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    tensor = transforms(img)\n",
    "    # next step adds a batch of 1 so image shape\n",
    "    # goes from 3, 224, 224 to 1, 3, 224, 224\n",
    "    tensor = tensor[None, :, :, :]\n",
    "    logits, softmax, arg_max = predict(model, tensor)\n",
    "    return img, idx_to_class[arg_max.item()], softmax[:,arg_max].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed16e0-f3b4-4459-ad6a-e70d675714dd",
   "metadata": {},
   "source": [
    "Try running this cell with different images selected from different folders and see what results you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542f437-ac04-46f7-ba0c-325bac5ffcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = 'data/val/bird/1018.jpg'\n",
    "idx_to_class = {v: k for k, v in train_ds.class_to_idx.items()}\n",
    "img, prediction, conf = predict_filepath(model, sample_img, val_transforms, idx_to_class)\n",
    "print(prediction, conf)\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f544b001-868e-477d-8948-8fb25530773b",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "These exercises are intended to help you understand this notebook further, you don't need to do them all or in sequence - just pick the ones that interest you the most. Or even explore how changing the notebook affects the results. **In the cases where you want to try training the model from scratch again then it is a good idea to reset the notebook and run all cells again**:\n",
    "***\n",
    "1. Can we use a different backbone? Some very simple changes would be to try a larger ResNet, the number 18 in the one we have used refers to the number of layers but there are versions with 34, 50, 101 and 152 layers. Have a look at the cell defining the model, specifically the lines:\n",
    "- `from torchvision.models import resnet18, resnet`\n",
    "- `self.backbone = nn.Sequential(*list(resnet18(weights=resnet.ResNet18_Weights.DEFAULT).children())[:-1])`\n",
    "\n",
    "***\n",
    "2. We have evaluated the model on the validation set and returned an overall accuracy score but does this represent the best way to validate the performance of the model? Is there any other metrics we could calculate on this dataset?\n",
    "\n",
    "Use this code to obtain all predictions and labels for the validation set and think about what else you could calculate:\n",
    "```python\n",
    "preds = []\n",
    "labels = []\n",
    "for x, y in val_dl:\n",
    "    logits, softmax, argmax = predict(model, x)\n",
    "    preds.extend(argmax.tolist())\n",
    "    labels.extend(y.tolist())\n",
    "```\n",
    "\n",
    "***\n",
    "3. What happens if the normalization steps are removed from the transform pipeline, how does this affect the values of `x` in the batches from the training dataloader? How does this affect the model training?\n",
    "\n",
    "***\n",
    "4. Are there any other transforms that could be added to the training transform pipeline - have a look [here](https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended) and try a few!\n",
    "\n",
    "\n",
    "***\n",
    "5. When we use the pretrained model we are 'cheating' a little bit - it has been trained on ImageNet and the image net dataset includes many animals including dogs and cats so the model actually already knows how to extract features. What happens if we don't use a pretrained model, take a look at this line in the model definition and modify it so we start with a completely fresh model:\n",
    "- `self.backbone = nn.Sequential(*list(resnet18(weights=resnet.ResNet18_Weights.DEFAULT).children())[:-1])`\n",
    "\n",
    "How does this change the accuracy achieved in 5 epochs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
